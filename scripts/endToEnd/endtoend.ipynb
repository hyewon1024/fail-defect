{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cc11f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Processing episodes:   0%|          | 0/505 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/505] Processing: success_episode1_steps320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   0%|          | 1/505 [00:00<05:16,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/505] Processing: success_episode2_steps316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   0%|          | 2/505 [00:01<04:50,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/505] Processing: success_episode3_steps306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|          | 3/505 [00:01<04:18,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/505] Processing: success_episode4_steps314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|          | 4/505 [00:02<04:05,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/505] Processing: success_episode5_steps311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|          | 5/505 [00:02<03:57,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/505] Processing: success_episode6_steps308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|          | 6/505 [00:02<03:52,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/505] Processing: success_episode7_steps329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|▏         | 7/505 [00:03<03:55,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/505] Processing: success_episode8_steps308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   2%|▏         | 8/505 [00:04<04:11,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/505] Processing: success_episode9_steps334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   2%|▏         | 9/505 [00:04<04:05,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/505] Processing: success_episode10_steps319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   2%|▏         | 10/505 [00:04<04:06,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/505] Processing: success_episode11_steps353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 103\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_npz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# 실행\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[43mprocess_all_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/AILAB-summer-school-2025/success_data_raw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/image_features(resnet18).npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[76], line 75\u001b[0m, in \u001b[0;36mprocess_all_episodes\u001b[0;34m(root_dir, output_npz)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(episode_dirs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Processing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     front_feat \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mep_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfront_view\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     top_feat   \u001b[38;5;241m=\u001b[39m extract_features(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ep_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_view\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     77\u001b[0m     wrist_feat \u001b[38;5;241m=\u001b[39m extract_features(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ep_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrist_view\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[76], line 53\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(image_dir)\u001b[0m\n\u001b[1;32m     51\u001b[0m     img_tensor \u001b[38;5;241m=\u001b[39m transform(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 53\u001b[0m         feat \u001b[38;5;241m=\u001b[39m \u001b[43mresnet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(feat\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(features, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:103\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m    102\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[0;32m--> 103\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m(out)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1920\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. 이미지를 resnet에 넣고 npz 파일로 저장\n",
    "datastructure\n",
    "{\n",
    "  'front_view': np.ndarray of shape [total_length, T, 512],\n",
    "  'top_view':   np.ndarray of shape [total_length, T, 512],\n",
    "  'wrist_view': np.ndarray of shape [total_length, T, 512],\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import models, transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ResNet18 feature extractor\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = torch.nn.Identity()  # remove final fc\n",
    "resnet18 = resnet18.to(device)\n",
    "resnet18.eval()\n",
    "#print(\"Weight hash:\", hash(str(resnet18.state_dict())))\n",
    "# Image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_features(image_dir):\n",
    "    def extract_index(filename):\n",
    "        match = re.search(r'front_view_(\\d+)\\.png', filename)\n",
    "        return int(match.group(1)) if match else -1\n",
    "\n",
    "    image_files = sorted(\n",
    "        [f for f in os.listdir(image_dir) if f.endswith(\".png\")],\n",
    "        key=extract_index\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for fname in image_files:\n",
    "        #print(fname)\n",
    "        img = Image.open(os.path.join(image_dir, fname)).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feat = resnet18(img_tensor)\n",
    "        features.append(feat.cpu().numpy()[0])\n",
    "\n",
    "    return np.stack(features, axis=0)  # shape: [T, 512]\n",
    "\n",
    "def get_episode_number(ep_dir_name):\n",
    "    match = re.search(r\"success_episode(\\d+)_steps(\\d+)\", ep_dir_name)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "def process_all_episodes(root_dir, output_npz):\n",
    "    front_list, top_list, wrist_list = [], [], []\n",
    "\n",
    "    episode_dirs = sorted(\n",
    "        [d for d in os.listdir(root_dir) if d.startswith(\"success_episode\")],\n",
    "        key=get_episode_number\n",
    "    )\n",
    "   \n",
    "    for i, ep_dir in enumerate(tqdm(episode_dirs, desc=\"Processing episodes\")):\n",
    "        ep_path = os.path.join(root_dir, ep_dir)\n",
    "        print(f\"[{i+1}/{len(episode_dirs)}] Processing: {ep_dir}\")\n",
    "\n",
    "        try:\n",
    "            front_feat = extract_features(os.path.join(ep_path, \"front_view\"))\n",
    "            top_feat   = extract_features(os.path.join(ep_path, \"top_view\"))\n",
    "            wrist_feat = extract_features(os.path.join(ep_path, \"wrist_view\"))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {ep_dir}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Optionally check consistency of T\n",
    "        T_f, T_t, T_w = len(front_feat), len(top_feat), len(wrist_feat)\n",
    "        if T_f != T_t or T_t != T_w:\n",
    "            print(f\"⚠️ Skipping {ep_dir} due to mismatched frame counts: front={T_f}, top={T_t}, wrist={T_w}\")\n",
    "            continue\n",
    "\n",
    "        front_list.append(front_feat)\n",
    "        top_list.append(top_feat)\n",
    "        wrist_list.append(wrist_feat)\n",
    "\n",
    "    # Save as object arrays (list of np.ndarray)\n",
    "    np.savez_compressed(\n",
    "        output_npz,\n",
    "        front_view=np.array(front_list, dtype=object),\n",
    "        top_view=np.array(top_list, dtype=object),\n",
    "        wrist_view=np.array(wrist_list, dtype=object)\n",
    "    )\n",
    "    print(f\"✅ Saved to {output_npz}\")\n",
    "\n",
    "\n",
    "# 실행\n",
    "process_all_episodes(\"/AILAB-summer-school-2025/success_data_raw\", \"dataset/image_features(resnet18).npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loaded file: /AILAB-summer-school-2025/scripts/make_dataset/image_features(resnet18).npz\n",
      "🔑 Keys in file: ['front_view', 'top_view', 'wrist_view']\n",
      "\n",
      "▶️ Key: 'front_view'\n",
      "   - Shape: (6,)\n",
      "   - Dtype: object\n",
      "   - Example values (first 1 row):\n",
      "1.7124043703079224\n",
      "\n",
      "▶️ Key: 'top_view'\n",
      "   - Shape: (6,)\n",
      "   - Dtype: object\n",
      "   - Example values (first 1 row):\n",
      "0.6083284616470337\n",
      "\n",
      "▶️ Key: 'wrist_view'\n",
      "   - Shape: (6,)\n",
      "   - Dtype: object\n",
      "   - Example values (first 1 row):\n",
      "2.733461856842041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "2. npz 파일 shape확인\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def inspect_image_features(npz_path):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    print(f\"📁 Loaded file: {npz_path}\")\n",
    "    print(f\"🔑 Keys in file: {list(data.keys())}\\n\")\n",
    "\n",
    "    for key in data.files:\n",
    "        array = data[key]\n",
    "        print(f\"▶️ Key: '{key}'\")\n",
    "        print(f\"   - Shape: {array.shape}\")\n",
    "        print(f\"   - Dtype: {array.dtype}\")\n",
    "        print(f\"   - Example values (first 1 row):\\n{len(array[0][0])}\")\n",
    "        print()\n",
    "\n",
    "# 사용 예시\n",
    "npz_file_path = \"/AILAB-summer-school-2025/scripts/make_dataset/image_features(resnet18).npz\"\n",
    "inspect_image_features(npz_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16bd9470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight hash: -1720887647942330811\n",
      "🔍 Checking episode: success_episode26_steps313, stride=5\n",
      "\n",
      "🔎 Checking view: front_view\n",
      "✅ front_view: All features match.\n",
      "\n",
      "🔎 Checking view: top_view\n",
      "✅ top_view: All features match.\n",
      "\n",
      "🔎 Checking view: wrist_view\n",
      "✅ wrist_view: All features match.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# ✅ ResNet18 정의 (512차원 출력)\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = torch.nn.Identity()\n",
    "resnet18.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resnet18 = resnet18.to(device)\n",
    "print(\"Weight hash:\", hash(str(resnet18.state_dict())))\n",
    "\n",
    "# ✅ 이미지 transform 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ✅ 설정\n",
    "npz_path = \"/AILAB-summer-school-2025/scripts/make_dataset/image_features(resnet18).npz\"\n",
    "success_data_raw = \"/AILAB-summer-school-2025/success_data_raw\"\n",
    "episode_idx = 25  # 검증할 에피소드 인덱스\n",
    "stride = 5       # 저장된 프레임 간격\n",
    "view_names = [\"front_view\", \"top_view\", \"wrist_view\"]\n",
    "\n",
    "# ✅ .npz 로딩\n",
    "data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "# ✅ 에피소드 디렉토리 정렬\n",
    "episode_dirs = sorted(\n",
    "    [d for d in os.listdir(success_data_raw) if d.startswith(\"success_episode\")],\n",
    "    key=lambda name: int(re.search(r\"success_episode(\\d+)\", name).group(1))\n",
    ")\n",
    "episode_name = episode_dirs[episode_idx]\n",
    "episode_path = os.path.join(success_data_raw, episode_name)\n",
    "\n",
    "print(f\"🔍 Checking episode: {episode_name}, stride={stride}\")\n",
    "\n",
    "# ✅ View 별 점검 루프\n",
    "for view in view_names:\n",
    "    print(f\"\\n🔎 Checking view: {view}\")\n",
    "    ep_feat_seq = data[view][episode_idx]  # shape [T, 512]\n",
    "    T = ep_feat_seq.shape[0]\n",
    "    view_dir = os.path.join(episode_path, view)\n",
    "\n",
    "    num_fail = 0\n",
    "    for i in range(T):\n",
    "        frame_number = i * stride\n",
    "        filename = f\"{view}_{frame_number}.png\"\n",
    "        filepath = os.path.join(view_dir, filename)\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"❌ Missing file: {filepath}\")\n",
    "            num_fail += 1\n",
    "            continue\n",
    "\n",
    "        image = Image.open(filepath).convert(\"RGB\")\n",
    "        img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat_from_resnet = resnet18(img_tensor).cpu().numpy().squeeze()\n",
    "\n",
    "        feat_from_npz = ep_feat_seq[i]\n",
    "        is_same = np.allclose(feat_from_npz, feat_from_resnet, atol=1e-6)\n",
    "\n",
    "        if not is_same:\n",
    "            diff = np.abs(feat_from_npz - feat_from_resnet)\n",
    "            print(f\"⚠️  Frame {i} (image {frame_number}.png): mismatch\")\n",
    "            print(f\"   → Max diff: {np.max(diff):.6f}, Mean diff: {np.mean(diff):.6f}\")\n",
    "            num_fail += 1\n",
    "\n",
    "    if num_fail == 0:\n",
    "        print(f\"✅ {view}: All features match.\")\n",
    "    else:\n",
    "        print(f\"❌ {view}: {num_fail}/{T} frames failed consistency check.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e397538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing view: front_view\n",
      "[INFO] View 'front_view': Collected 32858 features\n",
      "[INFO] PCA for front_view: Explained variance = 0.9478\n",
      "[INFO] Saved PCA model to /AILAB-summer-school-2025/scripts/make_dataset/pca_front_view.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing front_view: 100%|██████████| 505/505 [00:02<00:00, 222.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing view: top_view\n",
      "[INFO] View 'top_view': Collected 32858 features\n",
      "[INFO] PCA for top_view: Explained variance = 0.9659\n",
      "[INFO] Saved PCA model to /AILAB-summer-school-2025/scripts/make_dataset/pca_top_view.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing top_view: 100%|██████████| 505/505 [00:01<00:00, 272.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing view: wrist_view\n",
      "[INFO] View 'wrist_view': Collected 32858 features\n",
      "[INFO] PCA for wrist_view: Explained variance = 0.9397\n",
      "[INFO] Saved PCA model to /AILAB-summer-school-2025/scripts/make_dataset/pca_wrist_view.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing wrist_view: 100%|██████████| 505/505 [00:01<00:00, 286.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved PCA-compressed features (64-dim) to /AILAB-summer-school-2025/scripts/make_dataset/image_features_pca.npz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PCA 학습 및 적용 스크립트\n",
    "- 중요한 점: 각 view (front_view, top_view, wrist_view) 마다 개별적으로 PCA를 학습하고 적용함\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 설정 ===\n",
    "input_path = \"/AILAB-summer-school-2025/scripts/make_dataset/image_features(resnet18).npz\"\n",
    "output_path = \"/AILAB-summer-school-2025/scripts/make_dataset/image_features(pca).npz\"\n",
    "pca_dir = \"/AILAB-summer-school-2025/scripts/make_dataset\"\n",
    "\n",
    "views = [\"front_view\", \"top_view\", \"wrist_view\"]\n",
    "compressed_dim = 64\n",
    "\n",
    "# === 데이터 로드 ===\n",
    "data = np.load(input_path, allow_pickle=True)\n",
    "data = dict(data)\n",
    "\n",
    "# === PCA 적용 결과 저장용 리스트 ===\n",
    "compressed_data = {\n",
    "    \"front_view\": [],\n",
    "    \"top_view\": [],\n",
    "    \"wrist_view\": []\n",
    "}\n",
    "\n",
    "# === View별 PCA 학습 및 적용 ===\n",
    "for view in views:\n",
    "    print(f\"\\n[INFO] Processing view: {view}\")\n",
    "\n",
    "    # 전체 feature 수집\n",
    "    all_feats = []\n",
    "    for epi in data[view]:\n",
    "        for step_feat in epi:\n",
    "            all_feats.append(step_feat)\n",
    "    all_feats = np.stack(all_feats)  # shape: (N, 512)\n",
    "\n",
    "    print(f\"[INFO] View '{view}': Collected {all_feats.shape[0]} features\")\n",
    "\n",
    "    # PCA 학습\n",
    "    pca = PCA(n_components=compressed_dim)\n",
    "    pca.fit(all_feats)\n",
    "    print(f\"[INFO] PCA for {view}: Explained variance = {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "    # PCA 저장\n",
    "    pca_path = os.path.join(pca_dir, f\"pca_{view}.pkl\")\n",
    "    with open(pca_path, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "    print(f\"[INFO] Saved PCA model to {pca_path}\")\n",
    "\n",
    "    # PCA 변환 적용\n",
    "    for epi in tqdm(data[view], desc=f\"Compressing {view}\"):\n",
    "        compressed_epi = []\n",
    "        for step_feat in epi:\n",
    "            step_feat = step_feat.reshape(1, -1)  # shape: (1, 512)\n",
    "            compressed_feat = pca.transform(step_feat).flatten()  # shape: (64,)\n",
    "            compressed_epi.append(compressed_feat)\n",
    "        compressed_data[view].append(np.stack(compressed_epi))  # shape: (T, 64)\n",
    "\n",
    "# === npz로 저장 (dtype=object로 리스트 구조 유지) ===\n",
    "np.savez_compressed(\n",
    "    output_path,\n",
    "    front_view=np.array(compressed_data[\"front_view\"], dtype=object),\n",
    "    top_view=np.array(compressed_data[\"top_view\"], dtype=object),\n",
    "    wrist_view=np.array(compressed_data[\"wrist_view\"], dtype=object)\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Saved PCA-compressed features (64-dim) to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c39bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loaded file: /AILAB-summer-school-2025/scripts/make_dataset/image_features(pca).npz\n",
      "🔑 Keys in file: ['front_view', 'top_view', 'wrist_view']\n",
      "\n",
      "▶️ Key: 'front_view'\n",
      "   - Shape: (505,)\n",
      "   - Dtype: object\n",
      "   - Example values (first 1 row):\n",
      "1\n",
      "\n",
      "▶️ Key: 'top_view'\n",
      "   - Shape: (505,)\n",
      "   - Dtype: object\n",
      "   - Example values (first 1 row):\n",
      "1\n",
      "\n",
      "▶️ Key: 'wrist_view'\n",
      "   - Shape: (505,)\n",
      "   - Dtype: object\n",
      "   - Example values (first 1 row):\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "4. pca 파일 shape확인\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def inspect_image_features(npz_path):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    print(f\"📁 Loaded file: {npz_path}\")\n",
    "    print(f\"🔑 Keys in file: {list(data.keys())}\\n\")\n",
    "\n",
    "    for key in data.files:\n",
    "        array = data[key]\n",
    "        print(f\"▶️ Key: '{key}'\")\n",
    "        print(f\"   - Shape: {array.shape}\")\n",
    "        print(f\"   - Dtype: {array.dtype}\")\n",
    "        print(f\"   - Example values (first 1 row):\\n{len(array[:1])}\")\n",
    "        print()\n",
    "\n",
    "# 사용 예시\n",
    "npz_file_path = \"/AILAB-summer-school-2025/scripts/make_dataset/image_features(pca).npz\"\n",
    "inspect_image_features(npz_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
