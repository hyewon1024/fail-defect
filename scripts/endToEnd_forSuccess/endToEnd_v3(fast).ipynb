{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "apply resent18 (for success)\n",
    "\n",
    "structure\n",
    "data = {\n",
    "    \"success_episode{episode_number}_steps{episode_length//5+1}\":\n",
    "        [episode_length//5+1, 512*3] #front, top, wrist\n",
    "    }\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "# ====== 설정 ======\n",
    "ROOT_DIR = \"/home/user/Desktop/endToEnd_forSuccess/success_data_raw\"   # 입력 루트\n",
    "OUT_PATH = \"dataset/success_data_resnet18_robotX.pkl\"                    # 출력 파일\n",
    "VIEWS = (\"front\", \"top\", \"wrist\")                         # 처리할 뷰\n",
    "\n",
    "# 이미지 전처리 (요청 그대로)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std= [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# 에피소드 폴더명: success_episode{num}_steps{episode_length}\n",
    "EP_DIR_RE = re.compile(r\"^success_episode(\\d+)_steps(\\d+)$\")\n",
    "\n",
    "\n",
    "def parse_episode_dirname(name: str) -> Tuple[int, int]:\n",
    "    m = EP_DIR_RE.match(name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Invalid episode dir name: {name}\")\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "\n",
    "def list_episode_dirs(root: str) -> List[Tuple[int, int, str, str]]:\n",
    "    \"\"\"(ep_num, ep_len, abs_path, dir_name) 를 ep_num 오름차순으로 반환\"\"\"\n",
    "    eps = []\n",
    "    for name in os.listdir(root):\n",
    "        p = os.path.join(root, name)\n",
    "        if not os.path.isdir(p):\n",
    "            continue\n",
    "        if EP_DIR_RE.match(name):\n",
    "            ep_num, ep_len = parse_episode_dirname(name)\n",
    "            eps.append((ep_num, ep_len, p, name))\n",
    "    eps.sort(key=lambda x: x[0])\n",
    "    return eps\n",
    "\n",
    "\n",
    "def expected_steps(ep_len: int) -> List[int]:\n",
    "    \"\"\"0부터 (ep_len-1)까지 5 간격 스텝 (예: ep_len=320 -> [0,5,...,315], 총 64개)\"\"\"\n",
    "    return list(range(0, ep_len, 5))\n",
    "\n",
    "\n",
    "def img_path(ep_path: str, view: str, step: int) -> str:\n",
    "    \"\"\"뷰/스텝에 해당하는 이미지 경로\"\"\"\n",
    "    return os.path.join(ep_path, f\"{view}_view\", f\"{view}_view_{step}.png\")\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def extract_feature(model: nn.Module, path: str, device: torch.device) -> np.ndarray:\n",
    "    with Image.open(path).convert(\"RGB\") as img:\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        f = model(x).squeeze(0).detach().cpu().numpy().astype(np.float32)  # (512,)\n",
    "    return f\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 모델 준비 (ResNet18 + ImageNet 가중치, FC 제거 → 512차원)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    model.fc = nn.Identity()\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # 에피소드 스캔\n",
    "    episodes = list_episode_dirs(ROOT_DIR)\n",
    "    if not episodes:\n",
    "        raise RuntimeError(f\"No valid episode directories found under '{ROOT_DIR}'\")\n",
    "\n",
    "    data: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for ep_num, ep_len, ep_path, ep_name in episodes:\n",
    "        steps = expected_steps(ep_len)                 # 길이 = ep_len // 5\n",
    "        print(f\"[Episode {ep_num:>4}] {ep_name} -> steps expected: {len(steps)} (0..{ep_len-1} by 5)\")\n",
    "\n",
    "        # 파일 존재 검증(세 뷰 모두)\n",
    "        missing = []\n",
    "        for s in steps:\n",
    "            for v in VIEWS:\n",
    "                p = img_path(ep_path, v, s)\n",
    "                if not os.path.isfile(p):\n",
    "                    missing.append(p)\n",
    "        if missing:\n",
    "            # 어떤 에피소드에서든 필수 스텝이 비어있으면 바로 알려주고 중단\n",
    "            preview = \"\\n\".join(missing[:20])\n",
    "            tail = \"\" if len(missing) <= 20 else f\"\\n... (+{len(missing)-20} more)\"\n",
    "            raise FileNotFoundError(\n",
    "                f\"[{ep_name}] Missing required image files:\\n{preview}{tail}\"\n",
    "            )\n",
    "\n",
    "        # 특징 추출\n",
    "        episode_feats = []\n",
    "        for s in steps:\n",
    "            paths = [img_path(ep_path, v, s) for v in VIEWS]\n",
    "            feats = [extract_feature(model, p, device) for p in paths]  # 3 x (512,)\n",
    "            merged = np.concatenate(feats, axis=0)  # (1536,)\n",
    "            episode_feats.append(merged)\n",
    "            #print(f\"    - step {s:>4}  OK\")\n",
    "\n",
    "        arr = np.stack(episode_feats, axis=0)  # (T, 1536), T = ep_len // 5\n",
    "        key = f\"success_episode{ep_num}_steps{len(steps)}\"\n",
    "        data[key] = arr\n",
    "        print(f\"  -> Added episode: key='{key}', array shape={arr.shape}\")\n",
    "\n",
    "    # 저장\n",
    "    with open(OUT_PATH, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(f\"\\nSaved: {OUT_PATH}\")\n",
    "    print(f\"Total episodes: {len(data)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, re, sys, pickle\n",
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# ===== 설정 =====\n",
    "ROOT_DIR = \"success_data_raw\"   # 에피소드 디렉토리들의 상위 경로\n",
    "PKL_PATH = \"dataset/success_data_resnet18_robotX.pkl\"            # 기존 데이터셋 pkl (값 shape=(T,1536) 가정)\n",
    "OUT_PATH = \"dataset/success_data_resnet18_robotO.pkl\"                                           # None이면 PKL_PATH에 덮어씀. 아니면 새 경로 지정.\n",
    "\n",
    "# 디렉토리/키 패턴\n",
    "DIR_RE = re.compile(r\"^success_episode(\\d+)_steps(\\d+)$\")\n",
    "KEY_RE = re.compile(r\"^success_episode(\\d+)_steps(\\d+)$\")\n",
    "\n",
    "def err(msg: str):\n",
    "    print(f\"[ERROR] {msg}\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "def load_pkl(path: str) -> Dict[str, np.ndarray]:\n",
    "    if not os.path.isfile(path):\n",
    "        err(f\"PKL not found: {path}\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    if not isinstance(obj, dict):\n",
    "        err(\"PKL root must be a dict.\")\n",
    "    return obj\n",
    "\n",
    "def parse_key(key: str) -> Tuple[int, int]:\n",
    "    m = KEY_RE.match(key)\n",
    "    if not m:\n",
    "        err(f\"Unexpected key format: {key}\")\n",
    "    return int(m.group(1)), int(m.group(2))  # episode_num, key_steps\n",
    "\n",
    "def find_episode_dir(root: str, episode_num: int) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    같은 episode_num을 가진 디렉토리를 찾고 반환.\n",
    "    반환: (abs_path, raw_steps_from_dir)\n",
    "    주: 디렉토리명 steps는 '원 프레임 수'로 가정.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for name in os.listdir(root):\n",
    "        m = DIR_RE.match(name)\n",
    "        if not m:\n",
    "            continue\n",
    "        ep = int(m.group(1)); raw_steps = int(m.group(2))\n",
    "        if ep == episode_num:\n",
    "            candidates.append((os.path.join(root, name), raw_steps, name))\n",
    "    if not candidates:\n",
    "        err(f\"Episode dir not found under '{root}' for episode{episode_num}\")\n",
    "    if len(candidates) > 1:\n",
    "        # 에피소드 번호 충돌 방지: 가장 최근 수정시간 기준 선택\n",
    "        candidates.sort(key=lambda x: os.path.getmtime(x[0]), reverse=True)\n",
    "    return candidates[0][0], candidates[0][1]\n",
    "\n",
    "def load_ee_pose(ep_dir: str, expected_T: int) -> np.ndarray:\n",
    "    npz_path = os.path.join(ep_dir, \"robot_state.npz\")\n",
    "    if not os.path.isfile(npz_path):\n",
    "        err(f\"robot_state.npz not found: {npz_path}\")\n",
    "    state = np.load(npz_path, allow_pickle=True)\n",
    "    if \"EE_pose\" not in state:\n",
    "        err(f\"'EE_pose' key not found in {npz_path}\")\n",
    "    ee = state[\"EE_pose\"]\n",
    "    ee = np.asarray(ee, dtype=np.float32)\n",
    "    if ee.ndim != 2 or ee.shape[1] != 7:\n",
    "        err(f\"EE_pose must have shape (T,7). Got {ee.shape} at {npz_path}\")\n",
    "    if ee.shape[0] != expected_T:\n",
    "        err(f\"EE_pose T mismatch. Expected {expected_T}, got {ee.shape[0]} at {npz_path}\")\n",
    "    return ee\n",
    "\n",
    "def main():\n",
    "    data = load_pkl(PKL_PATH)\n",
    "\n",
    "    # 키가 하나도 없으면 종료\n",
    "    if not data:\n",
    "        err(\"PKL has no keys.\")\n",
    "\n",
    "    # 모든 키 확인 및 처리\n",
    "    updated: Dict[str, np.ndarray] = {}\n",
    "    for key, val in data.items():\n",
    "        ep_num, key_T = parse_key(key)\n",
    "\n",
    "        # 값 shape 검사 (T,1536)\n",
    "        arr = np.asarray(val)\n",
    "        if arr.ndim != 2 or arr.shape[1] != 1536:\n",
    "            err(f\"Value shape must be (T,1536). Key={key}, got {arr.shape}\")\n",
    "\n",
    "        # 디렉토리 찾기\n",
    "        ep_dir, raw_steps = find_episode_dir(ROOT_DIR, ep_num)\n",
    "\n",
    "        # 검증: 디렉 steps를 5로 나눈 몫 + 1 == key_T 이어야 함\n",
    "        derived_T = ((raw_steps -1) // 5) + 1\n",
    "        if derived_T != key_T:\n",
    "            err(\n",
    "                f\"steps check failed. Dir '{os.path.basename(ep_dir)}' has raw_steps={raw_steps} \"\n",
    "                f\"-> (raw//5)+1={derived_T}, but key steps={key_T}.\"\n",
    "            )\n",
    "\n",
    "        # episode 번호도 이미 일치 확인됨. EE_pose 로드 및 shape 검사\n",
    "        ee_pose = load_ee_pose(ep_dir, expected_T=key_T)\n",
    "\n",
    "        # concat -> (T, 1543)\n",
    "        if arr.shape[0] != key_T:\n",
    "            err(f\"T mismatch between value and key. Key={key}, value.shape[0]={arr.shape[0]}\")\n",
    "        merged = np.concatenate([arr.astype(np.float32, copy=False), ee_pose], axis=1)\n",
    "        if merged.shape != (key_T, 1543):\n",
    "            err(f\"Merged shape must be (T,1543). Got {merged.shape} for key={key}\")\n",
    "\n",
    "        updated[key] = merged\n",
    "        # 진행 상황 표시\n",
    "        print(f\"[OK] {key}: (T,1536) + (T,7) -> (T,1543) | dir={os.path.basename(ep_dir)}\")\n",
    "\n",
    "    # 저장\n",
    "    out_path = OUT_PATH or PKL_PATH\n",
    "    # 백업\n",
    "    if OUT_PATH is None:\n",
    "        bak = PKL_PATH + \".bak\"\n",
    "        try:\n",
    "            if os.path.exists(bak):\n",
    "                os.remove(bak)\n",
    "            os.rename(PKL_PATH, bak)\n",
    "            print(f\"[INFO] Backup saved: {bak}\")\n",
    "        except Exception as e:\n",
    "            err(f\"Failed to backup original PKL: {e}\")\n",
    "\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(updated, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"[DONE] Saved: {out_path} | keys={len(updated)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b15c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# ===== 설정 =====\n",
    "ROOT_DIR = \"success_data_raw\"   # 에피소드 디렉토리들의 상위 경로\n",
    "PKL_PATH = \"dataset/success_data_resnet18_robotO.pkl\"    \n",
    "\n",
    "# 정규식\n",
    "KEY_RE = re.compile(r\"^success_episode(\\d+)_steps(\\d+)$\")\n",
    "DIR_RE = re.compile(r\"^success_episode(\\d+)_steps(\\d+)$\")\n",
    "\n",
    "def parse_key(key):\n",
    "    m = KEY_RE.match(key)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Invalid key format: {key}\")\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def find_dir_for_episode(ep_num):\n",
    "    matches = []\n",
    "    for name in os.listdir(ROOT_DIR):\n",
    "        m = DIR_RE.match(name)\n",
    "        if m and int(m.group(1)) == ep_num:\n",
    "            matches.append(os.path.join(ROOT_DIR, name))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"Dir not found for episode {ep_num}\")\n",
    "    if len(matches) > 1:\n",
    "        print(f\"[WARN] Multiple dirs found for ep{ep_num}, using first: {matches[0]}\")\n",
    "    return matches[0]\n",
    "def main():\n",
    "    with open(PKL_PATH, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    all_ok = True\n",
    "    for key, arr in data.items():\n",
    "        ep_num, step_count = parse_key(key)\n",
    "\n",
    "        # 값 형상 확인\n",
    "        arr = np.asarray(arr)\n",
    "        if arr.shape[0] != step_count:\n",
    "            print(f\"[FAIL] {key}: array length mismatch {arr.shape[0]} != {step_count}\")\n",
    "            all_ok = False\n",
    "            continue\n",
    "\n",
    "        # 디렉토리 찾기\n",
    "        ep_dir = find_dir_for_episode(ep_num)\n",
    "\n",
    "        # EE_pose 로드\n",
    "        npz_path = os.path.join(ep_dir, \"robot_state.npz\")\n",
    "        if not os.path.isfile(npz_path):\n",
    "            print(f\"[FAIL] {key}: robot_state.npz not found in {ep_dir}\")\n",
    "            all_ok = False\n",
    "            continue\n",
    "\n",
    "        state = np.load(npz_path, allow_pickle=True)\n",
    "        if \"EE_pose\" not in state:\n",
    "            print(f\"[FAIL] {key}: 'EE_pose' key missing in {npz_path}\")\n",
    "            all_ok = False\n",
    "            continue\n",
    "\n",
    "        ee_pose = np.asarray(state[\"EE_pose\"], dtype=np.float32)\n",
    "        if ee_pose.shape != (step_count, 7):\n",
    "            print(f\"[FAIL] {key}: EE_pose shape mismatch {ee_pose.shape} != ({step_count}, 7)\")\n",
    "            all_ok = False\n",
    "            continue\n",
    "\n",
    "        # 마지막 7차원 값 비교\n",
    "        pkl_pose = arr[:, -7:].astype(np.float32)\n",
    "        if not np.allclose(pkl_pose, ee_pose, atol=1e-6):\n",
    "            print(f\"[FAIL] {key}: EE_pose values differ\")\n",
    "            all_ok = False\n",
    "        else:\n",
    "            print(f\"[OK]   {key}\")\n",
    "\n",
    "    if all_ok:\n",
    "        print(\"\\nAll episodes passed verification.\")\n",
    "    else:\n",
    "        print(\"\\nSome episodes failed verification.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d27703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, re, pickle, sys\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "IN_PATH  = \"dataset/success_data_resnet18_robotX.pkl\"\n",
    "OUT_PATH = \"dataset/success_data_resnet18_pca_robotX.pkl\"\n",
    "VIEWS    = (\"front\", \"top\", \"wrist\")\n",
    "KEY_RE   = re.compile(r\"^success_episode(\\d+)_steps(\\d+)$\")\n",
    "\n",
    "CHUNK = 8192  # 메모리 크면 줄여서 사용. 전체 한 번에 처리 가능하면 None로.\n",
    "def parse_key_nums(key: str) -> tuple[int, int]:\n",
    "    m = KEY_RE.match(key)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Invalid key format: {key}\")\n",
    "    return int(m.group(1)), int(m.group(2))  # (episode_num, steps_num)\n",
    "\n",
    "def load_pca(view: str):\n",
    "    p = f\"model/model_pca_{view}_view.pkl\"\n",
    "    if not os.path.isfile(p):\n",
    "        raise FileNotFoundError(f\"PCA model not found: {p}\")\n",
    "    with open(p, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    # 입력 차원 확인\n",
    "    n_in = getattr(model, \"n_features_in_\", 512)\n",
    "    if n_in != 512:\n",
    "        raise ValueError(f\"{view} PCA expects 512-d input, got {n_in}\")\n",
    "    # 출력 차원 사전 확인(가능할 때)\n",
    "    n_out = getattr(model, \"n_components_\", None)\n",
    "    if n_out is not None and n_out != 64:\n",
    "        raise ValueError(f\"{view} PCA expects 64-d output, got {n_out}\")\n",
    "    return model\n",
    "\n",
    "def pca_transform_chunked(X: np.ndarray, model, chunk: int | None) -> np.ndarray:\n",
    "    if chunk is None or X.shape[0] <= (chunk or 0):\n",
    "        Z = model.transform(X)\n",
    "    else:\n",
    "        out = []\n",
    "        for i in range(0, X.shape[0], chunk):\n",
    "            out.append(model.transform(X[i:i+chunk]))\n",
    "        Z = np.concatenate(out, axis=0)\n",
    "    if Z.shape[1] != 64:\n",
    "        raise RuntimeError(f\"PCA transform produced width {Z.shape[1]}, expected 64\")\n",
    "    return Z.astype(np.float32, copy=False)\n",
    "\n",
    "def main():\n",
    "    pca = {v: load_pca(v) for v in VIEWS}\n",
    "\n",
    "    with open(IN_PATH, \"rb\") as f:\n",
    "        data: Dict[str, np.ndarray] = pickle.load(f)\n",
    "    if not isinstance(data, dict):\n",
    "        raise TypeError(\"Input PKL must be dict {key: ndarray}.\")\n",
    "\n",
    "    out: Dict[str, np.ndarray] = {}\n",
    "    for key, arr in sorted(data.items(), key=lambda kv: parse_key_nums(kv[0])):\n",
    "        ep_num, steps = parse_key_nums(key)\n",
    "        arr = np.asarray(arr)\n",
    "        if arr.ndim != 2:\n",
    "            raise ValueError(f\"{key}: value must be 2D, got {arr.ndim}D.\")\n",
    "        T, D = arr.shape\n",
    "        if T != steps:\n",
    "            raise ValueError(f\"{key}: time length mismatch. value T={T} != key steps={steps}\")\n",
    "        if D < 1536:\n",
    "            raise ValueError(f\"{key}: expected >=1536 dims, got {D}\")\n",
    "        if np.isnan(arr[:, :1536]).any():\n",
    "            raise ValueError(f\"{key}: NaN detected in first 1536 dims\")\n",
    "\n",
    "        feat1536 = arr[:, :1536].astype(np.float32, copy=False)\n",
    "        f_front, f_top, f_wrist = feat1536[:, :512], feat1536[:, 512:1024], feat1536[:, 1024:1536]\n",
    "        if f_front.shape[1] != 512 or f_top.shape[1] != 512 or f_wrist.shape[1] != 512:\n",
    "            raise ValueError(f\"{key}: 512-split failed. got {[f_front.shape, f_top.shape, f_wrist.shape]}\")\n",
    "\n",
    "        z_front = pca_transform_chunked(f_front, pca[\"front\"], CHUNK)\n",
    "        z_top   = pca_transform_chunked(f_top,   pca[\"top\"],   CHUNK)\n",
    "        z_wrist = pca_transform_chunked(f_wrist, pca[\"wrist\"], CHUNK)\n",
    "\n",
    "        z = np.concatenate([z_front, z_top, z_wrist], axis=1)\n",
    "        if z.shape != (T, 192):\n",
    "            raise RuntimeError(f\"{key}: PCA concat shape expected {(T,192)}, got {z.shape}\")\n",
    "\n",
    "        out[key] = z\n",
    "        print(f\"[OK] {key}: {arr.shape} -> {z.shape}\")\n",
    "\n",
    "    with open(OUT_PATH, \"wb\") as f:\n",
    "        pickle.dump(out, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"\\nSaved: {OUT_PATH} | keys={len(out)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.set_printoptions(suppress=True)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# ===== 경로 =====\n",
    "PCA_PATH   = \"dataset/success_data_resnet18_pca_robotX.pkl\"  # (T,192)\n",
    "ROBOT_PATH = \"dataset/success_data_resnet18_robotX.pkl\"      # (T,1536+7) or (T,1543)\n",
    "OUT_PATH   = \"dataset/success_data_resnet18_pca_robotO.pkl\"  # (T,199) 저장 경로\n",
    "\n",
    "def main():\n",
    "    with open(PCA_PATH, \"rb\") as f:\n",
    "        pca_data = pickle.load(f)\n",
    "    with open(ROBOT_PATH, \"rb\") as f:\n",
    "        robot_data = pickle.load(f)\n",
    "\n",
    "    out = {}\n",
    "    for key, pca_arr in pca_data.items():\n",
    "        if key not in robot_data:\n",
    "            raise KeyError(f\"{key} not in robot_data PKL\")\n",
    "        pca_arr = np.asarray(pca_arr, dtype=np.float32)\n",
    "        rob_arr = np.asarray(robot_data[key], dtype=np.float32)\n",
    "\n",
    "        # 로봇 데이터 7차원 추출\n",
    "        if rob_arr.shape[1] < 7:\n",
    "            raise ValueError(f\"{key}: robot_data shape {rob_arr.shape} has no 7-dim tail\")\n",
    "        robot7 = rob_arr[:, -7:].astype(np.float32, copy=False)\n",
    "\n",
    "        if pca_arr.shape[0] != robot7.shape[0]:\n",
    "            raise ValueError(f\"{key}: step mismatch {pca_arr.shape[0]} vs {robot7.shape[0]}\")\n",
    "\n",
    "        merged = np.concatenate([pca_arr, robot7], axis=1)\n",
    "        if merged.shape != (pca_arr.shape[0], 192 + 7):\n",
    "            raise RuntimeError(f\"{key}: unexpected merged shape {merged.shape}\")\n",
    "        out[key] = merged\n",
    "        print(f\"[OK] {key}: {pca_arr.shape} + (T,7) -> {merged.shape}\")\n",
    "\n",
    "    with open(OUT_PATH, \"wb\") as f:\n",
    "        pickle.dump(out, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"\\nSaved: {OUT_PATH} | keys={len(out)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
