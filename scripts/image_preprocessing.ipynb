{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b8a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-Only Processor Config\n",
      "Views: ['front', 'top', 'wrist']\n",
      "ResNet Features: 512 per view\n",
      "Compressed Features: 64 per view\n",
      "Total Compressed: 192\n",
      "Device: cuda\n",
      "[INFO] ResNet18 feature extractor initialized on cuda\n",
      "[INFO] Found 506 episodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   0%|          | 0/506 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode230_steps306\n",
      "[INFO] Robot state shape: (62, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   0%|          | 1/506 [00:00<04:22,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode365_steps308\n",
      "[INFO] Robot state shape: (62, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   0%|          | 2/506 [00:01<05:08,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode149_steps307\n",
      "[INFO] Robot state shape: (62, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|          | 3/506 [00:01<05:29,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode481_steps323\n",
      "[INFO] Robot state shape: (65, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|          | 4/506 [00:02<05:45,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode395_steps313\n",
      "[INFO] Robot state shape: (63, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|          | 5/506 [00:03<05:53,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode38_steps359\n",
      "[INFO] Robot state shape: (72, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|          | 6/506 [00:04<06:15,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode305_steps315\n",
      "[INFO] Robot state shape: (63, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   1%|▏         | 7/506 [00:04<06:13,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode4_steps314\n",
      "[INFO] Robot state shape: (63, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   2%|▏         | 8/506 [00:05<06:11,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode267_steps347\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   2%|▏         | 9/506 [00:06<05:43,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode118_steps332\n",
      "[INFO] Robot state shape: (67, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   2%|▏         | 10/506 [00:06<05:43,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode157_steps308\n",
      "[INFO] Robot state shape: (62, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   2%|▏         | 11/506 [00:07<05:43,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode218_steps330\n",
      "[INFO] Robot state shape: (66, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   2%|▏         | 12/506 [00:08<05:55,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode119_steps313\n",
      "[INFO] Robot state shape: (63, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   3%|▎         | 13/506 [00:09<05:56,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode287_steps333\n",
      "[INFO] Robot state shape: (67, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   3%|▎         | 14/506 [00:09<06:07,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode113_steps324\n",
      "[INFO] Robot state shape: (65, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   3%|▎         | 15/506 [00:10<05:53,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode13_steps360\n",
      "[INFO] Robot state shape: (72, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   3%|▎         | 16/506 [00:11<06:16,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode443_steps309\n",
      "[INFO] Robot state shape: (62, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   3%|▎         | 17/506 [00:12<06:11,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode141_steps303\n",
      "[INFO] Robot state shape: (61, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   4%|▎         | 18/506 [00:12<06:05,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode333_steps305\n",
      "[INFO] Robot state shape: (61, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   4%|▍         | 19/506 [00:13<06:02,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode402_steps307\n",
      "[INFO] Robot state shape: (62, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   4%|▍         | 20/506 [00:14<06:01,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode145_steps325\n",
      "[INFO] Robot state shape: (65, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   4%|▍         | 21/506 [00:15<06:06,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode156_steps327\n",
      "[INFO] Robot state shape: (66, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   4%|▍         | 22/506 [00:15<05:45,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode339_steps306\n",
      "[INFO] Robot state shape: (62, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   5%|▍         | 23/506 [00:16<05:43,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode136_steps336\n",
      "[INFO] Robot state shape: (68, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   5%|▍         | 24/506 [00:17<05:31,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode447_steps325\n",
      "[INFO] Robot state shape: (65, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   5%|▍         | 25/506 [00:17<05:46,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode196_steps327\n",
      "[INFO] Robot state shape: (66, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   5%|▌         | 26/506 [00:18<05:51,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode15_steps313\n",
      "[INFO] Robot state shape: (63, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   5%|▌         | 27/506 [00:19<05:51,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode273_steps328\n",
      "[INFO] Robot state shape: (66, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   6%|▌         | 28/506 [00:20<05:57,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode293_steps321\n",
      "[INFO] Robot state shape: (65, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   6%|▌         | 29/506 [00:21<05:57,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode207_steps313\n",
      "[INFO] Robot state shape: (63, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   6%|▌         | 30/506 [00:21<05:52,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode292_steps329\n",
      "[INFO] Robot state shape: (66, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   6%|▌         | 31/506 [00:22<05:54,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/success_data_raw/success_episode338_steps317\n",
      "[INFO] Robot state shape: (64, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   6%|▌         | 31/506 [00:22<05:48,  1.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 190\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     epi_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(epi_dir)\n\u001b[0;32m--> 190\u001b[0m     episode_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepi_dir\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raw features + state\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     raw_episode_dicts[epi_name] \u001b[38;5;241m=\u001b[39m episode_dict\n\u001b[1;32m    192\u001b[0m     all_episode_features\u001b[38;5;241m.\u001b[39mappend(episode_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 109\u001b[0m, in \u001b[0;36mEpisodeProcessor.process_episode\u001b[0;34m(self, episode_dir)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m view \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mVIEWS:\n\u001b[1;32m    108\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(episode_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_view\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_view_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     view_feats\u001b[38;5;241m.\u001b[39mappend(feat)\n\u001b[1;32m    112\u001b[0m combined_img_feat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(view_feats)  \u001b[38;5;66;03m# [1536]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mEpisodeProcessor.extract_features\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     76\u001b[0m     image_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 78\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# success case \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class ImageOnlyConfig:\n",
    "    \"\"\"Configuration for image-only processing\"\"\"\n",
    "    \n",
    "    # ===== PATHS =====\n",
    "    IMAGE_FOLDER: str = \"success_traj_img\"\n",
    "    \n",
    "    OUTPUT_PATH: str = \"image_features.npz\"\n",
    "    PCA_MODEL_PATH: str = \"image_pca_models.pkl\"\n",
    "    \n",
    "    # ===== IMAGE PROCESSING =====\n",
    "    RESNET_FEATURE_DIM: int = 512  # ResNet18 final layer per view\n",
    "    VIEWS: List[str] = None\n",
    "    \n",
    "    # ===== PCA COMPRESSION =====\n",
    "    COMPRESSED_DIM: int = 64  # Final compressed dimension per view\n",
    "    TOTAL_COMPRESSED_DIM: int = 192  # 64 * 3 views\n",
    "    \n",
    "    # ===== MODEL =====\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    BATCH_SIZE: int = 32\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.VIEWS is None:\n",
    "            self.VIEWS = [\"front\", \"top\", \"wrist\"]\n",
    "        \n",
    "        print(f\"Image-Only Processor Config\")\n",
    "        print(f\"Views: {self.VIEWS}\")\n",
    "        print(f\"ResNet Features: {self.RESNET_FEATURE_DIM} per view\")\n",
    "        print(f\"Compressed Features: {self.COMPRESSED_DIM} per view\")\n",
    "        print(f\"Total Compressed: {self.TOTAL_COMPRESSED_DIM}\")\n",
    "        print(f\"Device: {self.DEVICE}\")\n",
    "\n",
    "class EpisodeProcessor:\n",
    "    def __init__(self, config: ImageOnlyConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.DEVICE)\n",
    "\n",
    "        # ResNet18 feature extractor\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.pca_models = {}\n",
    "        print(f\"[INFO] ResNet18 feature extractor initialized on {self.device}\")\n",
    "\n",
    "    def extract_features(self, image_path: str) -> np.ndarray:\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                features = self.model(image_tensor).view(1, -1)\n",
    "            return features.cpu().numpy().flatten()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to process {image_path}: {e}\")\n",
    "            return np.zeros(self.config.RESNET_FEATURE_DIM)\n",
    "\n",
    "    def process_episode(self, episode_dir: str) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Process a single episode directory\"\"\"\n",
    "        print(f\"[INFO] Processing episode: {episode_dir}\")\n",
    "        \n",
    "        # Load robot state\n",
    "        state_path = os.path.join(episode_dir, \"robot_state.npz\")\n",
    "        if not os.path.exists(state_path):\n",
    "            raise FileNotFoundError(f\"No robot_state.npz found in {episode_dir}\")\n",
    "        state_data = np.load(state_path)\n",
    "        state_key = list(state_data.keys())[0]  \n",
    "        robot_states = state_data[state_key]\n",
    "        print(f\"[INFO] Robot state shape: {robot_states.shape}\")\n",
    "\n",
    "        # Build timestep list\n",
    "        front_dir = os.path.join(episode_dir, \"front_view\")\n",
    "        timesteps = sorted([\n",
    "            int(f.split('_')[-1].replace('.png', ''))\n",
    "            for f in os.listdir(front_dir) if f.endswith('.png')\n",
    "        ])\n",
    "\n",
    "        features = []\n",
    "        for i, ts in enumerate(timesteps):\n",
    "            view_feats = []\n",
    "            for view in self.config.VIEWS:\n",
    "                img_path = os.path.join(episode_dir, f\"{view}_view\", f\"{view}_view_{ts}.png\")\n",
    "                feat = self.extract_features(img_path)\n",
    "                view_feats.append(feat)\n",
    "\n",
    "            combined_img_feat = np.concatenate(view_feats)  # [1536]\n",
    "            features.append(np.concatenate([combined_img_feat, robot_states[i]]))\n",
    "\n",
    "        return {\"observation\": np.vstack(features)}\n",
    "\n",
    "    def fit_pca(self, all_episode_features: List[np.ndarray]):\n",
    "        \"\"\"Fit PCA per view across all episodes\"\"\"\n",
    "        print(\"[INFO] Fitting PCA models...\")\n",
    "\n",
    "        total_img_dim = len(self.config.VIEWS) * self.config.RESNET_FEATURE_DIM\n",
    "        sample_feat = all_episode_features[0]\n",
    "        state_dim = sample_feat.shape[1] - total_img_dim\n",
    "        print(f\"[INFO] Detected state_dim = {state_dim}\")\n",
    "\n",
    "        view_features = {view: [] for view in self.config.VIEWS}\n",
    "\n",
    "        for episode_feat in all_episode_features:\n",
    "            img_feats = episode_feat[:, :-state_dim]\n",
    "            for i, view in enumerate(self.config.VIEWS):\n",
    "                start, end = i * self.config.RESNET_FEATURE_DIM, (i + 1) * self.config.RESNET_FEATURE_DIM\n",
    "                view_features[view].append(img_feats[:, start:end])\n",
    "\n",
    "        for view in self.config.VIEWS:\n",
    "            X = np.vstack(view_features[view])  # (N*T, 512)\n",
    "            pca = PCA(n_components=self.config.COMPRESSED_DIM)\n",
    "            pca.fit(X)\n",
    "            self.pca_models[view] = pca\n",
    "            print(f\"[INFO] {view} view PCA variance explained: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "    def compress_episode(self, episode_dict: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Apply PCA compression to images, concat state as-is\"\"\"\n",
    "        obs = episode_dict[\"observation\"]\n",
    "        total_img_dim = len(self.config.VIEWS) * self.config.RESNET_FEATURE_DIM\n",
    "        state_dim = obs.shape[1] - total_img_dim\n",
    "\n",
    "        img_feats, state_feats = obs[:, :-state_dim], obs[:, -state_dim:]\n",
    "\n",
    "        compressed_features = []\n",
    "        for row in img_feats:\n",
    "            comp_views = []\n",
    "            for i, view in enumerate(self.config.VIEWS):\n",
    "                start, end = i*self.config.RESNET_FEATURE_DIM, (i+1)*self.config.RESNET_FEATURE_DIM\n",
    "                view_feat = row[start:end].reshape(1, -1)\n",
    "                comp_views.append(self.pca_models[view].transform(view_feat).flatten())\n",
    "            compressed_features.append(np.concatenate(comp_views))\n",
    "\n",
    "        compressed_features = np.vstack(compressed_features)\n",
    "        final_obs = np.hstack([compressed_features, state_feats])  # PCA된 이미지 + 원본 state\n",
    "        return {\"observation\": final_obs}\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = ImageOnlyConfig(\n",
    "        IMAGE_FOLDER=\"/AILAB-summer-school-2025/success_data_raw\", # image path \n",
    "        OUTPUT_PATH=\"success_data_preprocessing\",\n",
    "        PCA_MODEL_PATH=\"pca_models.pkl\",\n",
    "        COMPRESSED_DIM=64\n",
    "    )\n",
    "    processor = EpisodeProcessor(config)\n",
    "\n",
    "    # Step 1. 전체 episode 폴더 탐색\n",
    "    episode_dirs = [\n",
    "        os.path.join(config.IMAGE_FOLDER, d)\n",
    "        for d in os.listdir(config.IMAGE_FOLDER)\n",
    "        if os.path.isdir(os.path.join(config.IMAGE_FOLDER, d))\n",
    "        and (\"success_\" in d or \"fail_\" in d)\n",
    "    ]\n",
    "    print(f\"[INFO] Found {len(episode_dirs)} episodes.\")\n",
    "\n",
    "    # Step 2. 각 episode feature 추출\n",
    "    all_episode_features = []\n",
    "    raw_episode_dicts = {}\n",
    "    for epi_dir in tqdm(episode_dirs, desc=\"Processing episodes\"):\n",
    "        try:\n",
    "            epi_name = os.path.basename(epi_dir)\n",
    "            episode_dict = processor.process_episode(epi_dir)  # raw features + state\n",
    "            raw_episode_dicts[epi_name] = episode_dict\n",
    "            all_episode_features.append(episode_dict[\"observation\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {epi_dir}: {e}\")\n",
    "\n",
    "    # Step 3. PCA 학습 (view별)\n",
    "    processor.fit_pca(all_episode_features)\n",
    "\n",
    "    # Step 4. PCA 압축 적용 및 저장\n",
    "    output_dir = \"model_path\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for epi_name, epi_dict in raw_episode_dicts.items():\n",
    "        compressed_dict = processor.compress_episode(epi_dict)\n",
    "\n",
    "        save_path = os.path.join(output_dir, f\"{epi_name}.npz\")\n",
    "        np.savez_compressed(save_path, **compressed_dict)\n",
    "        \n",
    "        print(f\"[INFO] Saved compressed episode: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3092a0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-Only Processor Config\n",
      "Views: ['front', 'top', 'wrist']\n",
      "ResNet Features: 512 per view\n",
      "Compressed Features: 64 per view\n",
      "Total Compressed: 192\n",
      "Device: cuda\n",
      "[INFO] ResNet18 feature extractor initialized on cuda\n",
      "[INFO] Found 22 episodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case1_missingGrasp/fail1_episode5_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   5%|▍         | 1/22 [00:00<00:16,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case1_missingGrasp/fail1_episode3_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:   9%|▉         | 2/22 [00:01<00:16,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case1_missingGrasp/fail1_episode2_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  14%|█▎        | 3/22 [00:02<00:15,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case1_missingGrasp/fail1_episode4_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  18%|█▊        | 4/22 [00:03<00:14,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case1_missingGrasp/fail1_episode1_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  23%|██▎       | 5/22 [00:04<00:13,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case4_release/fail4_episode2_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  27%|██▋       | 6/22 [00:04<00:12,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case4_release/fail4_episode4_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  32%|███▏      | 7/22 [00:05<00:12,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case4_release/fail4_episode1_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  36%|███▋      | 8/22 [00:06<00:11,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case4_release/fail4_episode5_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  41%|████      | 9/22 [00:07<00:10,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case4_release/fail4_episode3_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  45%|████▌     | 10/22 [00:08<00:09,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case2_outofcontrolPregrasp/fail2_episode4_step349_noise30\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  50%|█████     | 11/22 [00:08<00:08,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case2_outofcontrolPregrasp/fail2_episode1_step349_noise30\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  55%|█████▍    | 12/22 [00:09<00:08,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case2_outofcontrolPregrasp/fail2_episode5_step349_noise30\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  59%|█████▉    | 13/22 [00:10<00:07,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case2_outofcontrolPregrasp/fail2_episode2_step349_noise30\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  64%|██████▎   | 14/22 [00:11<00:06,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case2_outofcontrolPregrasp/fail2_episode3_step349_noise30\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  68%|██████▊   | 15/22 [00:12<00:05,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case3_outofcontrolMovetobin/fail3_episode5_step349_noise200\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  73%|███████▎  | 16/22 [00:12<00:04,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case3_outofcontrolMovetobin/fail3_episode4_step349_noise200\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  77%|███████▋  | 17/22 [00:13<00:03,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case3_outofcontrolMovetobin/fail3_episode3_step349_noise200\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  82%|████████▏ | 18/22 [00:13<00:02,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case3_outofcontrolMovetobin/fail3_episode1_step349_noise200\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  86%|████████▋ | 19/22 [00:14<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case3_outofcontrolMovetobin/fail3_episode2_step349_noise200\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  91%|█████████ | 20/22 [00:15<00:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case5_light/fail5_episode5_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes:  95%|█████████▌| 21/22 [00:16<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing episode: /AILAB-summer-school-2025/fail_data_raw/fail_case5_light/fail5_episode4_step349\n",
      "[INFO] Robot state shape: (70, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing episodes: 100%|██████████| 22/22 [00:16<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fitting PCA models...\n",
      "[INFO] Detected state_dim = 7\n",
      "[INFO] front view PCA variance explained: 0.938\n",
      "[INFO] top view PCA variance explained: 0.944\n",
      "[INFO] wrist view PCA variance explained: 0.935\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype1_episode5_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype1_episode3_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype1_episode2_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype1_episode4_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype1_episode1_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype4_episode2_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype4_episode4_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype4_episode1_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype4_episode5_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype4_episode3_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype2_episode4_step349_noise30.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype2_episode1_step349_noise30.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype2_episode5_step349_noise30.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype2_episode2_step349_noise30.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype2_episode3_step349_noise30.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype3_episode5_step349_noise200.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype3_episode4_step349_noise200.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype3_episode3_step349_noise200.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype3_episode1_step349_noise200.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype3_episode2_step349_noise200.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype5_episode5_step349.npz\n",
      "[INFO] Saved compressed episode: compressed_episodes/failtype5_episode4_step349.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# success case \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class ImageOnlyConfig:\n",
    "    \"\"\"Configuration for image-only processing\"\"\"\n",
    "    \n",
    "    # ===== PATHS =====\n",
    "    IMAGE_FOLDER: str = \"success_traj_img\"\n",
    "    \n",
    "    OUTPUT_PATH: str = \"image_features.npz\"\n",
    "    PCA_MODEL_PATH: str = \"image_pca_models.pkl\"\n",
    "    \n",
    "    # ===== IMAGE PROCESSING =====\n",
    "    RESNET_FEATURE_DIM: int = 512  # ResNet18 final layer per view\n",
    "    VIEWS: List[str] = None\n",
    "    \n",
    "    # ===== PCA COMPRESSION =====\n",
    "    COMPRESSED_DIM: int = 64  # Final compressed dimension per view\n",
    "    TOTAL_COMPRESSED_DIM: int = 192  # 64 * 3 views\n",
    "    \n",
    "    # ===== MODEL =====\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    BATCH_SIZE: int = 32\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.VIEWS is None:\n",
    "            self.VIEWS = [\"front\", \"top\", \"wrist\"]\n",
    "        \n",
    "        print(f\"Image-Only Processor Config\")\n",
    "        print(f\"Views: {self.VIEWS}\")\n",
    "        print(f\"ResNet Features: {self.RESNET_FEATURE_DIM} per view\")\n",
    "        print(f\"Compressed Features: {self.COMPRESSED_DIM} per view\")\n",
    "        print(f\"Total Compressed: {self.TOTAL_COMPRESSED_DIM}\")\n",
    "        print(f\"Device: {self.DEVICE}\")\n",
    "\n",
    "class EpisodeProcessor:\n",
    "    def __init__(self, config: ImageOnlyConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.DEVICE)\n",
    "\n",
    "        # ResNet18 feature extractor\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.pca_models = {}\n",
    "        print(f\"[INFO] ResNet18 feature extractor initialized on {self.device}\")\n",
    "\n",
    "    def extract_features(self, image_path: str) -> np.ndarray:\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                features = self.model(image_tensor).view(1, -1)\n",
    "            return features.cpu().numpy().flatten()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to process {image_path}: {e}\")\n",
    "            return np.zeros(self.config.RESNET_FEATURE_DIM)\n",
    "\n",
    "    def process_episode(self, episode_dir: str) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Process a single episode directory\"\"\"\n",
    "        print(f\"[INFO] Processing episode: {episode_dir}\")\n",
    "        \n",
    "        # Load robot state\n",
    "        state_path = os.path.join(episode_dir, \"robot_state.npz\")\n",
    "        if not os.path.exists(state_path):\n",
    "            raise FileNotFoundError(f\"No robot_state.npz found in {episode_dir}\")\n",
    "        state_data = np.load(state_path)\n",
    "        state_key = list(state_data.keys())[0]  \n",
    "        robot_states = state_data[state_key]\n",
    "        print(f\"[INFO] Robot state shape: {robot_states.shape}\")\n",
    "\n",
    "        # Build timestep list\n",
    "        front_dir = os.path.join(episode_dir, \"front_view\")\n",
    "        timesteps = sorted([\n",
    "            int(f.split('_')[-1].replace('.png', ''))\n",
    "            for f in os.listdir(front_dir) if f.endswith('.png')\n",
    "        ])\n",
    "\n",
    "        features = []\n",
    "        for i, ts in enumerate(timesteps):\n",
    "            view_feats = []\n",
    "            for view in self.config.VIEWS:\n",
    "                img_path = os.path.join(episode_dir, f\"{view}_view\", f\"{view}_view_{ts}.png\")\n",
    "                feat = self.extract_features(img_path)\n",
    "                view_feats.append(feat)\n",
    "\n",
    "            combined_img_feat = np.concatenate(view_feats)  # [1536]\n",
    "            features.append(np.concatenate([combined_img_feat, robot_states[i]]))\n",
    "\n",
    "        return {\"observation\": np.vstack(features)}\n",
    "\n",
    "    def fit_pca(self, all_episode_features: List[np.ndarray]):\n",
    "        \"\"\"Fit PCA per view across all episodes\"\"\"\n",
    "        print(\"[INFO] Fitting PCA models...\")\n",
    "\n",
    "        total_img_dim = len(self.config.VIEWS) * self.config.RESNET_FEATURE_DIM\n",
    "        sample_feat = all_episode_features[0]\n",
    "        state_dim = sample_feat.shape[1] - total_img_dim\n",
    "        print(f\"[INFO] Detected state_dim = {state_dim}\")\n",
    "\n",
    "        view_features = {view: [] for view in self.config.VIEWS}\n",
    "\n",
    "        for episode_feat in all_episode_features:\n",
    "            img_feats = episode_feat[:, :-state_dim]\n",
    "            for i, view in enumerate(self.config.VIEWS):\n",
    "                start, end = i * self.config.RESNET_FEATURE_DIM, (i + 1) * self.config.RESNET_FEATURE_DIM\n",
    "                view_features[view].append(img_feats[:, start:end])\n",
    "\n",
    "        for view in self.config.VIEWS:\n",
    "            X = np.vstack(view_features[view])  # (N*T, 512)\n",
    "            pca = PCA(n_components=self.config.COMPRESSED_DIM)\n",
    "            pca.fit(X)\n",
    "            self.pca_models[view] = pca\n",
    "            print(f\"[INFO] {view} view PCA variance explained: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "    def compress_episode(self, episode_dict: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Apply PCA compression to images, concat state as-is\"\"\"\n",
    "        obs = episode_dict[\"observation\"]\n",
    "        total_img_dim = len(self.config.VIEWS) * self.config.RESNET_FEATURE_DIM\n",
    "        state_dim = obs.shape[1] - total_img_dim\n",
    "\n",
    "        img_feats, state_feats = obs[:, :-state_dim], obs[:, -state_dim:]\n",
    "\n",
    "        compressed_features = []\n",
    "        for row in img_feats:\n",
    "            comp_views = []\n",
    "            for i, view in enumerate(self.config.VIEWS):\n",
    "                start, end = i*self.config.RESNET_FEATURE_DIM, (i+1)*self.config.RESNET_FEATURE_DIM\n",
    "                view_feat = row[start:end].reshape(1, -1)\n",
    "                comp_views.append(self.pca_models[view].transform(view_feat).flatten())\n",
    "            compressed_features.append(np.concatenate(comp_views))\n",
    "\n",
    "        compressed_features = np.vstack(compressed_features)\n",
    "        final_obs = np.hstack([compressed_features, state_feats])  # PCA된 이미지 + 원본 state\n",
    "        return {\"observation\": final_obs}\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def format_episode_name(epi_dir: str) -> str:\n",
    "    parts = epi_dir.split(os.sep)\n",
    "    base = parts[-1]       \n",
    "    parent = parts[-2] if len(parts) >= 2 else \"\"\n",
    "\n",
    "    if parent.startswith(\"fail_case\"):\n",
    "        failtype_num = ''.join([c for c in parent if c.isdigit()])\n",
    "        new_base = base.replace(f\"fail{failtype_num}_\", f\"failtype{failtype_num}_\")\n",
    "        return f\"{new_base}\"\n",
    "    else:\n",
    "        return base  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = ImageOnlyConfig(\n",
    "        IMAGE_FOLDER=\"/AILAB-summer-school-2025/fail_data_raw\", # image path \n",
    "        OUTPUT_PATH=\"final_episode_dicts.npz\",\n",
    "        PCA_MODEL_PATH=\"/AILAB-summer-school-2025/success_data_preprocessing/pca_weight/image_pca_models.pkl\",\n",
    "        COMPRESSED_DIM=64)\n",
    "\n",
    "    processor = EpisodeProcessor(config)\n",
    "\n",
    "    # Step 1. 전체 episode 폴더 탐색 (success/fail 구분)\n",
    "    episode_dirs = []\n",
    "    for root in os.listdir(config.IMAGE_FOLDER):\n",
    "        root_path = os.path.join(config.IMAGE_FOLDER, root)\n",
    "        if not os.path.isdir(root_path):\n",
    "            continue\n",
    "\n",
    "        if root.startswith(\"success_\"):  \n",
    "            episode_dirs.append(root_path)\n",
    "        elif root.startswith(\"fail_case\"):\n",
    "            for sub in os.listdir(root_path):\n",
    "                sub_path = os.path.join(root_path, sub)\n",
    "                if os.path.isdir(sub_path) and sub.startswith(\"fail\"):\n",
    "                    episode_dirs.append(sub_path)\n",
    "\n",
    "    print(f\"[INFO] Found {len(episode_dirs)} episodes.\")\n",
    "\n",
    "    # Step 2. 각 episode feature 추출\n",
    "    all_episode_features = []\n",
    "    raw_episode_dicts = {}\n",
    "    for epi_dir in tqdm(episode_dirs, desc=\"Processing episodes\"):\n",
    "        try:\n",
    "            epi_name = format_episode_name(epi_dir)\n",
    "            episode_dict = processor.process_episode(epi_dir)\n",
    "            raw_episode_dicts[epi_name] = episode_dict\n",
    "            all_episode_features.append(episode_dict[\"observation\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {epi_dir}: {e}\")\n",
    "\n",
    "    # Step 3. PCA 학습 (view별)\n",
    "    processor.fit_pca(all_episode_features)\n",
    "\n",
    "    # Step 4. PCA 압축 적용 및 저장\n",
    "    output_dir = \"compressed_episodes\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for epi_name, epi_dict in raw_episode_dicts.items():\n",
    "        compressed_dict = processor.compress_episode(epi_dict)\n",
    "        epi_name = epi_name.replace(\"/\", \"_\")\n",
    "        save_path = os.path.join(output_dir, f\"{epi_name}.npz\")\n",
    "        np.savez_compressed(save_path, **compressed_dict)\n",
    "        print(f\"[INFO] Saved compressed episode: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d8336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Test Config Loaded. Views = ['front', 'top', 'wrist']\n",
      "[INFO] Loaded PCA models from /AILAB-summer-school-2025/success_data_preprocessing/pca_weight/image_pca_models.pkl\n",
      "[INFO] front_view_15.png (front) compressed to shape (64,)\n",
      "[INFO] front_view_55.png (front) compressed to shape (64,)\n",
      "\n",
      "[INFO] ===== Compressed Features =====\n",
      "File: front_view_15.png, Feature shape: [ 4.72703    -1.072504   -0.87296116 -0.43789268  2.0988083  -0.6866239\n",
      "  0.04479933 -0.02085042  0.09806824 -1.0059185   0.06412601 -0.780879\n",
      "  0.91437745 -0.40677467 -0.37339795 -0.27510643  0.8253875  -0.9524193\n",
      "  0.756865   -0.53242755  0.5253252  -0.5092813   0.30531973 -0.21605682\n",
      " -0.21085715  0.12881082 -0.15477014  0.66942155 -0.865785    0.1336565\n",
      "  0.08154511  0.3179567   0.4893713  -0.1618681  -0.6881063   0.11584532\n",
      "  0.3137312   0.12497234  0.3625562   0.9273153   0.18536142 -0.10525858\n",
      " -0.2437501  -0.14380884 -0.15398502  0.51048946 -0.7326085   0.10164288\n",
      "  0.06678283  0.302735    0.8178067   0.85976624  0.10156906 -0.05368221\n",
      "  0.62407947  0.07917106  0.09732765 -0.07599688 -0.4062903  -0.5428035\n",
      " -0.25548804 -0.01486945  0.24452291 -0.49419993]\n",
      "------------------------------------------------------------\n",
      "File: front_view_55.png, Feature shape: [-1.4692161   0.47473103 -3.6847625  -2.3853884  -0.04707207  3.013295\n",
      " -0.7241138   1.2139374  -1.7278862  -0.6524806   2.365489    1.1003165\n",
      "  1.8879197  -0.7866771  -1.3792366   0.45198536  0.6731801  -1.1536255\n",
      " -0.5074055   1.8435049   0.3875022  -0.7833703   0.20109503 -0.4509064\n",
      " -0.07176733  0.02585405  0.8805187   0.7236474  -0.20479834 -0.08588028\n",
      " -0.39414656 -0.5622712   0.6822057  -0.47596407 -0.3497138   0.09653449\n",
      "  0.28993964  0.41890478  0.4737121   1.026169   -0.07003765 -0.6381036\n",
      " -0.16599679 -1.0514703   0.05790162 -0.08494055  0.25990212  0.32313356\n",
      " -0.0840683  -0.09915704  0.87092805  0.876271    0.55505633  0.50745595\n",
      "  0.75859094  0.06872988  0.15535384  0.10271198 -0.22064757 -1.1395195\n",
      " -0.6741791   0.6818478   0.38031426 -0.26470727]\n",
      "------------------------------------------------------------\n",
      "\n",
      "[INFO] Pairwise Feature Comparison\n",
      "front_view_15.png vs front_view_55.png:\n",
      "  Cosine Similarity = 0.0597\n",
      "  Euclidean Distance = 10.1360\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from typing import Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TestConfig:\n",
    "    TEST_FOLDER: str = \"\" #이미지 폴더 경로   put this folder 2 images\n",
    "    PCA_MODEL_PATH: str = \"pca_models.pkl\" #load path\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    VIEWS: list = None\n",
    "    RESNET_FEATURE_DIM: int = 512\n",
    "    COMPRESSED_DIM: int = 64\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.VIEWS is None:\n",
    "            self.VIEWS = [\"front\", \"top\", \"wrist\"]\n",
    "        print(f\"[INFO] Test Config Loaded. Views = {self.VIEWS}\")\n",
    "\n",
    "class TestProcessor:\n",
    "    def __init__(self, config: TestConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.DEVICE)\n",
    "\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        with open(config.PCA_MODEL_PATH, \"rb\") as f:\n",
    "            self.pca_models = pickle.load(f)\n",
    "        print(f\"[INFO] Loaded PCA models from {config.PCA_MODEL_PATH}\")\n",
    "\n",
    "    def extract_feature(self, image_path: str) -> np.ndarray:\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                feat = self.model(image_tensor).view(1, -1)\n",
    "            return feat.cpu().numpy().flatten()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not process {image_path}: {e}\")\n",
    "            return np.zeros(self.config.RESNET_FEATURE_DIM)\n",
    "\n",
    "    def process_test_folder(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Process all images in test folder and output compressed features\"\"\"\n",
    "        results = {}\n",
    "        for fname in os.listdir(self.config.TEST_FOLDER):\n",
    "            if not fname.endswith(\".png\"):\n",
    "                continue\n",
    "\n",
    "            fpath = os.path.join(self.config.TEST_FOLDER, fname)\n",
    "\n",
    "            # 파일명에서 view 추출 (front, top, wrist 중 하나)\n",
    "            view = None\n",
    "            for v in self.config.VIEWS:\n",
    "                if v in fname.lower():\n",
    "                    view = v\n",
    "                    break\n",
    "\n",
    "            if view is None:\n",
    "                print(f\"[WARN] No matching view found for {fname}, skipping\")\n",
    "                continue\n",
    "\n",
    "            feat = self.extract_feature(fpath)\n",
    "            pca_model = self.pca_models[view]\n",
    "            comp_feat = pca_model.transform(feat.reshape(1, -1)).flatten()\n",
    "\n",
    "            results[fname] = comp_feat\n",
    "            print(f\"[INFO] {fname} ({view}) compressed to shape {comp_feat.shape}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = TestConfig(\n",
    "        TEST_FOLDER=\"/AILAB-summer-school-2025/test\",              \n",
    "        PCA_MODEL_PATH=\"/AILAB-summer-school-2025/success_data_preprocessing/pca_weight/image_pca_models.pkl\"  #load path\n",
    "    )\n",
    "    tester = TestProcessor(config)\n",
    "\n",
    "    results = tester.process_test_folder()\n",
    "\n",
    "    print(\"\\n[INFO] ===== Compressed Features =====\")\n",
    "    for fname, comp_feat in results.items():\n",
    "        print(f\"File: {fname}, Feature shape: {comp_feat}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    if len(results) > 1:\n",
    "        names = list(results.keys())\n",
    "        feats = np.vstack(list(results.values()))\n",
    "\n",
    "        # Cosine 유사도\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        cos_sim = cosine_similarity(feats)\n",
    "        # Uclidean Distance\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "        euclid_dist = euclidean_distances(feats)\n",
    "\n",
    "        print(\"\\n[INFO] Pairwise Feature Comparison\")\n",
    "        for i in range(len(names)):\n",
    "            for j in range(i + 1, len(names)):\n",
    "                print(f\"{names[i]} vs {names[j]}:\")\n",
    "                print(f\"  Cosine Similarity = {cos_sim[i, j]:.4f}\")\n",
    "                print(f\"  Euclidean Distance = {euclid_dist[i, j]:.4f}\")\n",
    "                print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
